{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os, re, networkx\n",
    "from konlpy.utils import pprint\n",
    "#from ckonlpy.tag import Twitter\n",
    "#tw = Twitter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RawSentence: #문자열에 대한 처리\n",
    "    def __init__(self, textIter):\n",
    "        if type(textIter) == str: \n",
    "            self.textIter = textIter.split('\\n')\n",
    "        else: \n",
    "            self.textIter = textIter\n",
    "        self.rgxSplitter = re.compile('([.!?:](?:[\"\\']|(?![0-9])))')\n",
    " \n",
    "    def __iter__(self):\n",
    "        for line in self.textIter:\n",
    "            ch = self.rgxSplitter.split(line)\n",
    "            for s in map(lambda a, b: a + b, ch[::2], ch[1::2]):\n",
    "                if not s: \n",
    "                    continue\n",
    "                yield s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RawSentenceReader:\n",
    "    def __init__(self, filepath):\n",
    "        self.filepath = filepath\n",
    "        self.rgxSplitter = re.compile('([.!?:](?:[\"\\']|(?![0-9])))')\n",
    " \n",
    "    def __iter__(self):\n",
    "        for line in open(self.filepath, encoding='utf-8'):\n",
    "            ch = self.rgxSplitter.split(line)\n",
    "            for s in map(lambda a, b: a + b, ch[::2], ch[1::2]):\n",
    "                if not s: \n",
    "                    continue\n",
    "                yield s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RawTagger:\n",
    "    def __init__(self, textIter, tagger = None):\n",
    "        if tagger:\n",
    "            self.tagger = tagger\n",
    "        else :\n",
    "            from konlpy.tag import Twitter\n",
    "            self.tagger = Twitter()\n",
    "        if type(textIter) == str:\n",
    "            self.textIter = textIter.split('\\n')\n",
    "        else: \n",
    "            self.textIter = textIter\n",
    "        self.rgxSplitter = re.compile('([.!?:](?:[\"\\']|(?![0-9])))')\n",
    " \n",
    "    def __iter__(self):\n",
    "        for line in self.textIter:\n",
    "            ch = self.rgxSplitter.split(line)\n",
    "            for s in map(lambda a,b:a+b, ch[::2], ch[1::2]):\n",
    "                if not s: \n",
    "                    continue\n",
    "                yield self.tagger.pos(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RawTaggerReader:\n",
    "    def __init__(self, filepath, tagger = None):\n",
    "        if tagger:\n",
    "            self.tagger = tagger\n",
    "        else :\n",
    "            from konlpy.tag import Twitter\n",
    "            self.tagger = Twitter()\n",
    "        self.filepath = filepath\n",
    "        self.rgxSplitter = re.compile('([.!?:](?:[\"\\']|(?![0-9])))')\n",
    " \n",
    "    def __iter__(self):\n",
    "        for line in open(self.filepath, encoding='utf-8'):\n",
    "            ch = self.rgxSplitter.split(line)\n",
    "            for s in map(lambda a,b:a+b, ch[::2], ch[1::2]):\n",
    "                if not s: continue\n",
    "                yield self.tagger.pos(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextRank:\n",
    "    def __init__(self, **kargs):\n",
    "        self.graph = None\n",
    "        self.window = kargs.get('window', 5)\n",
    "        self.coef = kargs.get('coef', 1.0)\n",
    "        self.threshold = kargs.get('threshold', 0.005)\n",
    "        self.dictCount = {}\n",
    "        self.dictBiCount = {}\n",
    "        self.dictNear = {}\n",
    "        self.nTotal = 0\n",
    " \n",
    " \n",
    "    def load(self, sentenceIter, wordFilter = None):\n",
    "        def insertPair(a, b):\n",
    "            if a > b: a, b = b, a\n",
    "            elif a == b: return\n",
    "            self.dictBiCount[a, b] = self.dictBiCount.get((a, b), 0) + 1\n",
    " \n",
    "        def insertNearPair(a, b):\n",
    "            self.dictNear[a, b] = self.dictNear.get((a, b), 0) + 1\n",
    " \n",
    "        for sent in sentenceIter:\n",
    "            for i, word in enumerate(sent):\n",
    "                if wordFilter and not wordFilter(word): continue\n",
    "                self.dictCount[word] = self.dictCount.get(word, 0) + 1\n",
    "                self.nTotal += 1\n",
    "                if i - 1 >= 0 and (not wordFilter or wordFilter(sent[i-1])): insertNearPair(sent[i-1], word)\n",
    "                if i + 1 < len(sent) and (not wordFilter or wordFilter(sent[i+1])): insertNearPair(word, sent[i+1])\n",
    "                for j in range(i+1, min(i+self.window+1, len(sent))):\n",
    "                    if wordFilter and not wordFilter(sent[j]): continue\n",
    "                    if sent[j] != word: insertPair(word, sent[j])\n",
    " \n",
    "    def loadSents(self, sentenceIter, tokenizer = None):\n",
    "        import math\n",
    "        def similarity(a, b):\n",
    "            n = len(a.intersection(b))\n",
    "            return n / float(len(a) + len(b) - n) / (math.log(len(a)+1) * math.log(len(b)+1))\n",
    " \n",
    "        if not tokenizer: rgxSplitter = re.compile('[\\\\s.,:;-?!()\"\\']+')\n",
    "        sentSet = []\n",
    "        for sent in filter(None, sentenceIter):\n",
    "            if type(sent) == str:\n",
    "                if tokenizer: s = set(filter(None, tokenizer(sent)))\n",
    "                else: s = set(filter(None, rgxSplitter.split(sent)))\n",
    "            else: s = set(sent)\n",
    "            if len(s) < 2: continue\n",
    "            self.dictCount[len(self.dictCount)] = sent\n",
    "            sentSet.append(s)\n",
    " \n",
    "        for i in range(len(self.dictCount)):\n",
    "            for j in range(i+1, len(self.dictCount)):\n",
    "                s = similarity(sentSet[i], sentSet[j])\n",
    "                if s < self.threshold: continue\n",
    "                self.dictBiCount[i, j] = s\n",
    " \n",
    "    def getPMI(self, a, b):\n",
    "        import math\n",
    "        co = self.dictNear.get((a, b), 0)\n",
    "        if not co: return None\n",
    "        return math.log(float(co) * self.nTotal / self.dictCount[a] / self.dictCount[b])\n",
    " \n",
    "    def getI(self, a):\n",
    "        import math\n",
    "        if a not in self.dictCount: return None\n",
    "        return math.log(self.nTotal / self.dictCount[a])\n",
    " \n",
    "    def build(self):\n",
    "        self.graph = networkx.Graph()\n",
    "        self.graph.add_nodes_from(self.dictCount.keys())\n",
    "        for (a, b), n in self.dictBiCount.items():\n",
    "            self.graph.add_edge(a, b, weight = n*self.coef + (1-self.coef))\n",
    " \n",
    "    def rank(self):\n",
    "        return networkx.pagerank(self.graph, weight='weight')\n",
    " \n",
    "    def extract(self, ratio = 0.1):\n",
    "        ranks = self.rank()\n",
    "        cand = sorted(ranks, key=ranks.get, reverse=True)[:int(len(ranks) * ratio)]\n",
    "        pairness = {}\n",
    "        startOf = {}\n",
    "        tuples = {}\n",
    "        complex_pmi = []\n",
    "        for k in cand:\n",
    "            tuples[(k,)] = self.getI(k) * ranks[k]\n",
    "            for l in cand:\n",
    "                if k == l: continue\n",
    "                pmi = self.getPMI(k, l)\n",
    "                if pmi: pairness[k, l] = pmi\n",
    " \n",
    "        for (k, l) in sorted(pairness, key=pairness.get, reverse=True):\n",
    "            temp = (k[0]+l[0], pairness[k, l])\n",
    "            complex_pmi.append(temp)\n",
    "            if k not in startOf: startOf[k] = (k, l)\n",
    "        return complex_pmi\n",
    "\n",
    "        '''for (k, l), v in pairness.items():\n",
    "            pmis = v\n",
    "            rs = ranks[k] * ranks[l]\n",
    "            path = (k, l)\n",
    "            tuples[path] = pmis / (len(path) - 1) * rs ** (1 / len(path)) * len(path)\n",
    "            last = l\n",
    "            while last in startOf and len(path) < 7:\n",
    "                if last in path: break\n",
    "                pmis += pairness[startOf[last]]\n",
    "                last = startOf[last][1]\n",
    "                rs *= ranks[last]\n",
    "                path += (last,)\n",
    "                tuples[path] = pmis / (len(path) - 1) * rs ** (1 / len(path)) * len(path)\n",
    " \n",
    "        used = set()\n",
    "        both = {}\n",
    "        for k in sorted(tuples, key=tuples.get, reverse=True):\n",
    "            if used.intersection(set(k)): continue\n",
    "            both[k] = tuples[k]\n",
    "            for w in k: used.add(w)\n",
    " \n",
    "        #for k in cand:\n",
    "        #    if k not in used or True: both[k] = ranks[k] * self.getI(k)\n",
    " \n",
    "        return both'''\n",
    " \n",
    "    def summarize(self, ratio = 0.333):\n",
    "        r = self.rank()\n",
    "        ks = sorted(r, key=r.get, reverse=True)[:int(len(r)*ratio)]\n",
    "        return ' '.join(map(lambda k:self.dictCount[k], sorted(ks)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os, re, sqlite3, io\n",
    "import pandas as pd\n",
    "from pandas import Series, DataFrame\n",
    "from dateutil.parser import parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = sqlite3.connect(\"./data/data.db\")\n",
    "cursor = con.cursor()\n",
    "con.text_factory = bytes\n",
    "df = pd.read_sql(\"SELECT * FROM content\", con, index_col=None)\n",
    "df = df.iloc[1:-1]\n",
    "corpus = df.to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(content):\n",
    "    #content = re.sub('[a-zA-Z]', '', content)\n",
    "    content = re.sub('/(<([^>])>)/ig', '', content)\n",
    "    content = re.sub('\\\\t', '', content)\n",
    "    content = re.sub('[\\{\\}\\[\\]\\/?.,;:|\\)*~`!^\\-_+<>@\\#$%&\\\\\\=\\(\\'\\\")]', '', content)\n",
    "    content = ' '.join(content.split())\n",
    "    \n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def origin_tokenizing(content): #입력 문장 리스트를 토크나이징한다. Kkma는 pos로 분리하고, OS, NNG, NP만 빼내는 것이 좋다.\n",
    "    featurelist = []\n",
    "    tw = Twitter()\n",
    "    for term in tw.pos(content):\n",
    "        if term[1] == 'Noun':\n",
    "            featurelist.append(term[0])\n",
    "        elif term[1] == 'Alpha':\n",
    "            featurelist.append(term[0])\n",
    "        elif term[1] == 'Verb':\n",
    "            featurelist.append(term[0])\n",
    "    return featurelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45 'NoneType' object has no attribute 'decode'\n",
      "141 'NoneType' object has no attribute 'decode'\n",
      "146 'NoneType' object has no attribute 'decode'\n",
      "211 'NoneType' object has no attribute 'decode'\n"
     ]
    }
   ],
   "source": [
    "utf_corpus = []\n",
    "for i, data in enumerate(corpus):\n",
    "    try:\n",
    "        dd = dict()\n",
    "        dd['title'] = data['title'].decode('utf-8')\n",
    "        dd['content'] = data['content'].decode('utf-8')\n",
    "        dd['link'] = data['link'].decode('utf-8')\n",
    "        dd['publish'] = data['publish'].decode('utf-8')\n",
    "        dd['provider'] = data['provider'].decode('utf-8')\n",
    "        dd['tokens'] = origin_tokenizing(preprocessing(dd['content']))\n",
    "    except Exception as err:\n",
    "        print(i, err)\n",
    "    finally:\n",
    "        utf_corpus.append(dd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'content': b'',\n",
       " 'link': b'https://www.cnet.co.kr/view/?no=20150513161050',\n",
       " 'provider': b'cnet',\n",
       " 'publish': None,\n",
       " 'title': b''}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[45]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del utf_corpus[45] #리스트 인덱스 삭제하는법\n",
    "#del utf_corpus[141]\n",
    "#del utf_corpus[146]\n",
    "#del utf_corpus[211]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Docu:\n",
    "    def __init__(self, content, link, provider, publish, title, tokens):\n",
    "        self.content = content\n",
    "        self.link = link\n",
    "        self.provider = provider\n",
    "        self.publish = parse(publish)\n",
    "        self.title = title\n",
    "        self.tokens = tokens\n",
    "    def __repr__(self):\n",
    "        return repr((self.content, self.link, self.provider, self.publish, self.title, self.tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "docments = list()\n",
    "for i, data in enumerate(utf_corpus):\n",
    "    docments.append(Docu(data['content'], data['link'], data['provider'], data['publish'], data['title'], data['tokens']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = sorted(docments, key=lambda docu: docu.publish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015-05-04\n",
      "2015-05-04\n",
      "2015-05-06\n",
      "2015-05-06\n",
      "2015-05-06\n",
      "2015-05-06\n",
      "2015-05-06\n",
      "2015-05-06\n",
      "2015-05-07\n",
      "2015-05-07\n",
      "2015-05-07\n",
      "2015-05-08\n",
      "2015-05-11\n",
      "2015-05-12\n",
      "2015-05-12\n",
      "2015-05-12\n",
      "2015-05-12\n",
      "2015-05-12\n",
      "2015-05-13\n",
      "2015-05-14\n",
      "2015-05-14\n",
      "2015-05-14\n",
      "2015-05-14\n",
      "2015-05-15\n",
      "2015-05-15\n",
      "2015-05-15\n",
      "2015-05-15\n",
      "2015-05-18\n",
      "2015-05-18\n",
      "2015-05-18\n",
      "2015-05-18\n",
      "2015-05-19\n",
      "2015-05-19\n",
      "2015-05-19\n",
      "2015-05-20\n",
      "2015-05-20\n",
      "2015-05-20\n",
      "2015-05-20\n",
      "2015-05-20\n",
      "2015-05-20\n",
      "2015-05-21\n",
      "2015-05-21\n",
      "2015-05-21\n",
      "2015-05-22\n",
      "2015-05-22\n",
      "2015-05-22\n",
      "2015-05-26\n",
      "2015-05-26\n",
      "2015-05-28\n",
      "2015-05-28\n",
      "2015-05-28\n",
      "2015-05-28\n",
      "2015-05-29\n",
      "2015-05-29\n",
      "2015-05-29\n",
      "2015-05-29\n",
      "2015-05-29\n",
      "2015-06-01\n",
      "2015-06-01\n",
      "2015-06-01\n",
      "2015-06-02\n",
      "2015-06-02\n",
      "2015-06-02\n",
      "2015-06-02\n",
      "2015-06-02\n",
      "2015-06-02\n",
      "2015-06-02\n",
      "2015-06-03\n",
      "2015-06-03\n",
      "2015-06-03\n",
      "2015-06-03\n",
      "2015-06-03\n",
      "2015-06-03\n",
      "2015-06-04\n",
      "2015-06-04\n",
      "2015-06-04\n",
      "2015-06-04\n",
      "2015-06-05\n",
      "2015-06-05\n",
      "2015-06-05\n",
      "2015-06-05\n",
      "2015-06-08\n",
      "2015-06-08\n",
      "2015-06-08\n",
      "2015-06-09\n",
      "2015-06-09\n",
      "2015-06-09\n",
      "2015-06-09\n",
      "2015-06-09\n",
      "2015-06-09\n",
      "2015-06-10\n",
      "2015-06-10\n",
      "2015-06-10\n",
      "2015-06-11\n",
      "2015-06-11\n",
      "2015-06-11\n",
      "2015-06-11\n",
      "2015-06-11\n",
      "2015-06-11\n",
      "2015-06-11\n",
      "2015-06-12\n",
      "2015-06-12\n",
      "2015-06-12\n",
      "2015-06-12\n",
      "2015-06-15\n",
      "2015-06-15\n",
      "2015-06-15\n",
      "2015-06-15\n",
      "2015-06-15\n",
      "2015-06-15\n",
      "2015-06-16\n",
      "2015-06-16\n",
      "2015-06-16\n",
      "2015-06-17\n",
      "2015-06-17\n",
      "2015-06-17\n",
      "2015-06-18\n",
      "2015-06-18\n",
      "2015-06-18\n",
      "2015-06-19\n",
      "2015-06-19\n",
      "2015-06-19\n",
      "2015-06-19\n",
      "2015-06-22\n",
      "2015-06-23\n",
      "2015-06-23\n",
      "2015-06-24\n",
      "2015-06-24\n",
      "2015-06-25\n",
      "2015-06-25\n",
      "2015-06-25\n",
      "2015-06-25\n",
      "2015-06-25\n",
      "2015-06-26\n",
      "2015-06-29\n",
      "2015-06-29\n",
      "2015-06-29\n",
      "2015-06-30\n",
      "2015-06-30\n",
      "2015-06-30\n",
      "2015-07-01\n",
      "2015-07-01\n",
      "2015-07-01\n",
      "2015-07-02\n",
      "2015-07-03\n",
      "2015-07-03\n",
      "2015-07-06\n",
      "2015-07-06\n",
      "2015-07-06\n",
      "2015-07-06\n",
      "2015-07-07\n",
      "2015-07-08\n",
      "2015-07-08\n",
      "2015-07-08\n",
      "2015-07-08\n",
      "2015-07-09\n",
      "2015-07-09\n",
      "2015-07-09\n",
      "2015-07-09\n",
      "2015-07-10\n",
      "2015-07-10\n",
      "2015-07-10\n",
      "2015-07-10\n",
      "2015-07-13\n",
      "2015-07-13\n",
      "2015-07-13\n",
      "2015-07-13\n",
      "2015-07-13\n",
      "2015-07-13\n",
      "2015-07-14\n",
      "2015-07-14\n",
      "2015-07-14\n",
      "2015-07-15\n",
      "2015-07-15\n",
      "2015-07-15\n",
      "2015-07-15\n",
      "2015-07-16\n",
      "2015-07-16\n",
      "2015-07-16\n",
      "2015-07-16\n",
      "2015-07-16\n",
      "2015-07-16\n",
      "2015-07-16\n",
      "2015-07-17\n",
      "2015-07-20\n",
      "2015-07-20\n",
      "2015-07-20\n",
      "2015-07-21\n",
      "2015-07-21\n",
      "2015-07-21\n",
      "2015-07-21\n",
      "2015-07-21\n",
      "2015-07-21\n",
      "2015-07-21\n",
      "2015-07-22\n",
      "2015-07-22\n",
      "2015-07-22\n",
      "2015-07-22\n",
      "2015-07-22\n",
      "2015-07-22\n",
      "2015-07-23\n",
      "2015-07-23\n",
      "2015-07-23\n",
      "2015-07-23\n",
      "2015-07-24\n",
      "2015-07-24\n",
      "2015-07-24\n",
      "2015-07-24\n",
      "2015-07-24\n",
      "2015-07-24\n",
      "2015-07-24\n",
      "2015-07-27\n",
      "2015-07-27\n",
      "2015-07-27\n",
      "2015-07-27\n",
      "2015-07-28\n",
      "2015-07-28\n",
      "2015-07-28\n",
      "2015-07-28\n",
      "2015-07-28\n",
      "2015-07-28\n",
      "2015-07-28\n",
      "2015-07-28\n",
      "2015-07-28\n",
      "2015-07-28\n",
      "2015-07-30\n",
      "2015-07-31\n",
      "2015-07-31\n",
      "2015-07-31\n",
      "2015-07-31\n",
      "2015-08-01\n",
      "2015-08-03\n",
      "2015-08-03\n",
      "2015-08-03\n",
      "2015-08-03\n",
      "2015-08-03\n",
      "2015-08-04\n",
      "2015-08-04\n",
      "2015-08-04\n",
      "2015-08-05\n",
      "2015-08-05\n",
      "2015-08-05\n",
      "2015-08-05\n",
      "2015-08-05\n",
      "2015-08-05\n",
      "2015-08-05\n",
      "2015-08-06\n",
      "2015-08-06\n",
      "2015-08-06\n",
      "2015-08-06\n",
      "2015-08-06\n",
      "2015-08-07\n",
      "2015-08-07\n",
      "2015-08-10\n",
      "2015-08-10\n",
      "2015-08-10\n",
      "2015-08-11\n",
      "2015-08-11\n",
      "2015-08-11\n",
      "2015-08-12\n",
      "2015-08-12\n",
      "2015-08-13\n",
      "2015-08-13\n",
      "2015-08-13\n",
      "2015-08-14\n",
      "2015-08-14\n",
      "2015-08-17\n",
      "2015-08-19\n"
     ]
    }
   ],
   "source": [
    "#정렬 확인\n",
    "#for data in result:\n",
    "#    print(data.publish.date())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "predata = result[0]\n",
    "datecorpus = {}\n",
    "datecorpus[str(predata.publish.date())] = {'string' : predata.content + \"\\n\"}\n",
    "\n",
    "for i in range(1, len(result)):\n",
    "    if result[i].publish.date() == predata.publish.date():\n",
    "        try:\n",
    "            datecorpus[str(result[i].publish.date())]['string'] += str(result[i].content + \"\\n\")\n",
    "            #print(result[i].publish.date())\n",
    "        except KeyError as e:\n",
    "            datecorpus[str(result[i].publish.date())] = {'string' : str(result[i].content + \"\\n\")}\n",
    "            #print(e)\n",
    "    predata = result[i]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 57.1 ms\n",
      "59\n",
      "Wall time: 75.4 ms\n",
      "147\n",
      "Wall time: 31.7 ms\n",
      "39\n",
      "Wall time: 68.9 ms\n",
      "130\n",
      "Wall time: 47.6 ms\n",
      "85\n",
      "Wall time: 37.7 ms\n",
      "76\n",
      "Wall time: 60.5 ms\n",
      "107\n",
      "Wall time: 29.8 ms\n",
      "49\n",
      "Wall time: 79.9 ms\n",
      "177\n",
      "Wall time: 24.3 ms\n",
      "36\n",
      "Wall time: 37.2 ms\n",
      "53\n",
      "Wall time: 11.9 ms\n",
      "26\n",
      "Wall time: 38.2 ms\n",
      "78\n",
      "Wall time: 83.3 ms\n",
      "212\n",
      "Wall time: 49.6 ms\n",
      "97\n",
      "Wall time: 145 ms\n",
      "283\n",
      "Wall time: 80.3 ms\n",
      "130\n",
      "Wall time: 52.6 ms\n",
      "125\n",
      "Wall time: 31.7 ms\n",
      "53\n",
      "Wall time: 53.6 ms\n",
      "105\n",
      "Wall time: 300 ms\n",
      "497\n",
      "Wall time: 49.1 ms\n",
      "76\n",
      "Wall time: 177 ms\n",
      "229\n",
      "Wall time: 45.6 ms\n",
      "80\n",
      "Wall time: 123 ms\n",
      "266\n",
      "Wall time: 47.1 ms\n",
      "52\n",
      "Wall time: 84.3 ms\n",
      "162\n",
      "Wall time: 20.8 ms\n",
      "47\n",
      "Wall time: 64 ms\n",
      "143\n",
      "Wall time: 25.3 ms\n",
      "47\n",
      "Wall time: 35.3 ms\n",
      "77\n",
      "Wall time: 133 ms\n",
      "214\n",
      "Wall time: 31.7 ms\n",
      "69\n",
      "Wall time: 93.7 ms\n",
      "167\n",
      "Wall time: 104 ms\n",
      "184\n",
      "Wall time: 11.4 ms\n",
      "30\n",
      "Wall time: 40.7 ms\n",
      "105\n",
      "Wall time: 40.2 ms\n",
      "89\n",
      "Wall time: 64.5 ms\n",
      "110\n",
      "Wall time: 30.8 ms\n",
      "72\n",
      "Wall time: 79.9 ms\n",
      "121\n",
      "Wall time: 56.5 ms\n",
      "115\n",
      "Wall time: 65.5 ms\n",
      "137\n",
      "Wall time: 181 ms\n",
      "351\n",
      "Wall time: 28.3 ms\n",
      "64\n",
      "Wall time: 84.8 ms\n",
      "189\n",
      "Wall time: 62.5 ms\n",
      "139\n",
      "Wall time: 55.6 ms\n",
      "105\n",
      "Wall time: 73 ms\n",
      "202\n",
      "Wall time: 133 ms\n",
      "254\n",
      "Wall time: 97.7 ms\n",
      "220\n",
      "Wall time: 45.2 ms\n",
      "61\n",
      "Wall time: 91.2 ms\n",
      "219\n",
      "Wall time: 49.6 ms\n",
      "78\n",
      "Wall time: 81.8 ms\n",
      "172\n",
      "Wall time: 124 ms\n",
      "185\n",
      "Wall time: 94.7 ms\n",
      "129\n",
      "Wall time: 30.8 ms\n",
      "51\n",
      "Wall time: 44.1 ms\n",
      "95\n",
      "Wall time: 13.4 ms\n",
      "29\n",
      "Wall time: 38.2 ms\n",
      "64\n",
      "Wall time: 46.6 ms\n",
      "68\n"
     ]
    }
   ],
   "source": [
    "stopword = set([('있', 'Verb'), ('하', 'Verb'), ('되', 'Verb'), ('없', 'Verb')])\n",
    "for date in list(datecorpus.keys()):\n",
    "    tr = TextRank(window=1, coef=1, threshold = 5)\n",
    "    %time tr.load(RawTagger(datecorpus[date]['string']), lambda w: w not in stopword and (w[1] in ('Noun', 'Adjective', 'Alpha', 'Number')))\n",
    "    tr.build()\n",
    "    datecorpus[date]['pmi'] = tr.extract(0.4)\n",
    "    print(len(datecorpus[date]['pmi']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "origin_datecorpus = datecorpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "datecorpus = origin_datecorpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204\n",
      "241\n",
      "366\n",
      "448\n",
      "518\n",
      "617\n",
      "664\n",
      "830\n",
      "862\n",
      "907\n",
      "930\n",
      "999\n",
      "1192\n",
      "1273\n",
      "1507\n",
      "1623\n",
      "1717\n",
      "1763\n",
      "1834\n",
      "2258\n",
      "2316\n",
      "2508\n",
      "2577\n",
      "2772\n",
      "2811\n",
      "2947\n",
      "2987\n",
      "3090\n",
      "3126\n",
      "3181\n",
      "3337\n",
      "3389\n",
      "3520\n",
      "3675\n",
      "3699\n",
      "3773\n",
      "3840\n",
      "3922\n",
      "3975\n",
      "4054\n",
      "4134\n",
      "4239\n",
      "4495\n",
      "4534\n",
      "4662\n",
      "4782\n",
      "4851\n",
      "5000\n",
      "5214\n",
      "5362\n",
      "5406\n",
      "5572\n",
      "5631\n",
      "5743\n",
      "5887\n",
      "6007\n",
      "6040\n",
      "6113\n",
      "6129\n",
      "6172\n",
      "6223\n"
     ]
    }
   ],
   "source": [
    "#다시 짜자 좀 쉬고...\n",
    "a = list(datecorpus.keys())\n",
    "predate = datecorpus[a[0]]\n",
    "for i in range(1, len(a)):\n",
    "    for c_word in datecorpus[a[i]]['pmi']:\n",
    "        for j, pre_c_word in enumerate(predate['pmi']):\n",
    "            if str(pre_c_word[0]) == str(c_word[0]):\n",
    "                del predate['pmi'][j]\n",
    "    datecorpus[a[i]]['pmi'] += predate['pmi']\n",
    "    print(len(datecorpus[a[i]]['pmi']))\n",
    "    predate = datecorpus[a[i]]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(year,pop)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
