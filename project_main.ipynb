{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os, re, networkx\n",
    "from konlpy.utils import pprint\n",
    "#from ckonlpy.tag import Twitter\n",
    "#tw = Twitter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RawSentence: #문자열에 대한 처리\n",
    "    def __init__(self, textIter):\n",
    "        if type(textIter) == str: \n",
    "            self.textIter = textIter.split('\\n')\n",
    "        else: \n",
    "            self.textIter = textIter\n",
    "        self.rgxSplitter = re.compile('([.!?:](?:[\"\\']|(?![0-9])))')\n",
    " \n",
    "    def __iter__(self):\n",
    "        for line in self.textIter:\n",
    "            ch = self.rgxSplitter.split(line)\n",
    "            for s in map(lambda a, b: a + b, ch[::2], ch[1::2]):\n",
    "                if not s: \n",
    "                    continue\n",
    "                yield s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RawSentenceReader:\n",
    "    def __init__(self, filepath):\n",
    "        self.filepath = filepath\n",
    "        self.rgxSplitter = re.compile('([.!?:](?:[\"\\']|(?![0-9])))')\n",
    " \n",
    "    def __iter__(self):\n",
    "        for line in open(self.filepath, encoding='utf-8'):\n",
    "            ch = self.rgxSplitter.split(line)\n",
    "            for s in map(lambda a, b: a + b, ch[::2], ch[1::2]):\n",
    "                if not s: \n",
    "                    continue\n",
    "                yield s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RawTagger:\n",
    "    def __init__(self, textIter, tagger = None):\n",
    "        if tagger:\n",
    "            self.tagger = tagger\n",
    "        else :\n",
    "            from konlpy.tag import Twitter\n",
    "            self.tagger = Twitter()\n",
    "        if type(textIter) == str:\n",
    "            self.textIter = textIter.split('\\n')\n",
    "        else: \n",
    "            self.textIter = textIter\n",
    "        self.rgxSplitter = re.compile('([.!?:](?:[\"\\']|(?![0-9])))')\n",
    " \n",
    "    def __iter__(self):\n",
    "        for line in self.textIter:\n",
    "            ch = self.rgxSplitter.split(line)\n",
    "            for s in map(lambda a,b:a+b, ch[::2], ch[1::2]):\n",
    "                if not s: \n",
    "                    continue\n",
    "                yield self.tagger.pos(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RawTaggerReader:\n",
    "    def __init__(self, filepath, tagger = None):\n",
    "        if tagger:\n",
    "            self.tagger = tagger\n",
    "        else :\n",
    "            from konlpy.tag import Twitter\n",
    "            self.tagger = Twitter()\n",
    "        self.filepath = filepath\n",
    "        self.rgxSplitter = re.compile('([.!?:](?:[\"\\']|(?![0-9])))')\n",
    " \n",
    "    def __iter__(self):\n",
    "        for line in open(self.filepath, encoding='utf-8'):\n",
    "            ch = self.rgxSplitter.split(line)\n",
    "            for s in map(lambda a,b:a+b, ch[::2], ch[1::2]):\n",
    "                if not s: continue\n",
    "                yield self.tagger.pos(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextRank:\n",
    "    def __init__(self, **kargs):\n",
    "        self.graph = None\n",
    "        self.window = kargs.get('window', 5)\n",
    "        self.coef = kargs.get('coef', 1.0)\n",
    "        self.threshold = kargs.get('threshold', 0.005)\n",
    "        self.dictCount = {}\n",
    "        self.dictBiCount = {}\n",
    "        self.dictNear = {}\n",
    "        self.nTotal = 0\n",
    " \n",
    " \n",
    "    def load(self, sentenceIter, wordFilter = None):\n",
    "        def insertPair(a, b):\n",
    "            if a > b: a, b = b, a\n",
    "            elif a == b: return\n",
    "            self.dictBiCount[a, b] = self.dictBiCount.get((a, b), 0) + 1\n",
    " \n",
    "        def insertNearPair(a, b):\n",
    "            self.dictNear[a, b] = self.dictNear.get((a, b), 0) + 1\n",
    " \n",
    "        for sent in sentenceIter:\n",
    "            for i, word in enumerate(sent):\n",
    "                if wordFilter and not wordFilter(word): continue\n",
    "                self.dictCount[word] = self.dictCount.get(word, 0) + 1\n",
    "                self.nTotal += 1\n",
    "                if i - 1 >= 0 and (not wordFilter or wordFilter(sent[i-1])): insertNearPair(sent[i-1], word)\n",
    "                if i + 1 < len(sent) and (not wordFilter or wordFilter(sent[i+1])): insertNearPair(word, sent[i+1])\n",
    "                for j in range(i+1, min(i+self.window+1, len(sent))):\n",
    "                    if wordFilter and not wordFilter(sent[j]): continue\n",
    "                    if sent[j] != word: insertPair(word, sent[j])\n",
    " \n",
    "    def loadSents(self, sentenceIter, tokenizer = None):\n",
    "        import math\n",
    "        def similarity(a, b):\n",
    "            n = len(a.intersection(b))\n",
    "            return n / float(len(a) + len(b) - n) / (math.log(len(a)+1) * math.log(len(b)+1))\n",
    " \n",
    "        if not tokenizer: rgxSplitter = re.compile('[\\\\s.,:;-?!()\"\\']+')\n",
    "        sentSet = []\n",
    "        for sent in filter(None, sentenceIter):\n",
    "            if type(sent) == str:\n",
    "                if tokenizer: s = set(filter(None, tokenizer(sent)))\n",
    "                else: s = set(filter(None, rgxSplitter.split(sent)))\n",
    "            else: s = set(sent)\n",
    "            if len(s) < 2: continue\n",
    "            self.dictCount[len(self.dictCount)] = sent\n",
    "            sentSet.append(s)\n",
    " \n",
    "        for i in range(len(self.dictCount)):\n",
    "            for j in range(i+1, len(self.dictCount)):\n",
    "                s = similarity(sentSet[i], sentSet[j])\n",
    "                if s < self.threshold: continue\n",
    "                self.dictBiCount[i, j] = s\n",
    " \n",
    "    def getPMI(self, a, b):\n",
    "        import math\n",
    "        co = self.dictNear.get((a, b), 0)\n",
    "        if not co: return None\n",
    "        return math.log(float(co) * self.nTotal / self.dictCount[a] / self.dictCount[b])\n",
    " \n",
    "    def getI(self, a):\n",
    "        import math\n",
    "        if a not in self.dictCount: return None\n",
    "        return math.log(self.nTotal / self.dictCount[a])\n",
    " \n",
    "    def build(self):\n",
    "        self.graph = networkx.Graph()\n",
    "        self.graph.add_nodes_from(self.dictCount.keys())\n",
    "        for (a, b), n in self.dictBiCount.items():\n",
    "            self.graph.add_edge(a, b, weight = n*self.coef + (1-self.coef))\n",
    " \n",
    "    def rank(self):\n",
    "        return networkx.pagerank(self.graph, weight='weight')\n",
    " \n",
    "    def extract(self, ratio = 0.1):\n",
    "        ranks = self.rank()\n",
    "        cand = sorted(ranks, key=ranks.get, reverse=True)[:int(len(ranks) * ratio)]\n",
    "        pairness = {}\n",
    "        startOf = {}\n",
    "        tuples = {}\n",
    "        complex_pmi = []\n",
    "        for k in cand:\n",
    "            tuples[(k,)] = self.getI(k) * ranks[k]\n",
    "            for l in cand:\n",
    "                if k == l: continue\n",
    "                pmi = self.getPMI(k, l)\n",
    "                if pmi: pairness[k, l] = pmi\n",
    " \n",
    "        for (k, l) in sorted(pairness, key=pairness.get, reverse=True):\n",
    "            temp = (k[0]+l[0], pairness[k, l])\n",
    "            complex_pmi.append(temp)\n",
    "            if k not in startOf: startOf[k] = (k, l)\n",
    "        return complex_pmi\n",
    "\n",
    "        '''for (k, l), v in pairness.items():\n",
    "            pmis = v\n",
    "            rs = ranks[k] * ranks[l]\n",
    "            path = (k, l)\n",
    "            tuples[path] = pmis / (len(path) - 1) * rs ** (1 / len(path)) * len(path)\n",
    "            last = l\n",
    "            while last in startOf and len(path) < 7:\n",
    "                if last in path: break\n",
    "                pmis += pairness[startOf[last]]\n",
    "                last = startOf[last][1]\n",
    "                rs *= ranks[last]\n",
    "                path += (last,)\n",
    "                tuples[path] = pmis / (len(path) - 1) * rs ** (1 / len(path)) * len(path)\n",
    " \n",
    "        used = set()\n",
    "        both = {}\n",
    "        for k in sorted(tuples, key=tuples.get, reverse=True):\n",
    "            if used.intersection(set(k)): continue\n",
    "            both[k] = tuples[k]\n",
    "            for w in k: used.add(w)\n",
    " \n",
    "        #for k in cand:\n",
    "        #    if k not in used or True: both[k] = ranks[k] * self.getI(k)\n",
    " \n",
    "        return both'''\n",
    " \n",
    "    def summarize(self, ratio = 0.333):\n",
    "        r = self.rank()\n",
    "        ks = sorted(r, key=r.get, reverse=True)[:int(len(r)*ratio)]\n",
    "        return ' '.join(map(lambda k:self.dictCount[k], sorted(ks)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os, re, sqlite3, io\n",
    "import pandas as pd\n",
    "from pandas import Series, DataFrame\n",
    "from dateutil.parser import parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = sqlite3.connect(\"./data/data.db\")\n",
    "cursor = con.cursor()\n",
    "con.text_factory = bytes\n",
    "df = pd.read_sql(\"SELECT * FROM content\", con, index_col=None)\n",
    "df = df.iloc[1:-1]\n",
    "corpus = df.to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(content):\n",
    "    #content = re.sub('[a-zA-Z]', '', content)\n",
    "    content = re.sub('/(<([^>])>)/ig', '', content)\n",
    "    content = re.sub('\\\\t', '', content)\n",
    "    content = re.sub('[\\{\\}\\[\\]\\/?.,;:|\\)*~`!^\\-_+<>@\\#$%&\\\\\\=\\(\\'\\\")]', '', content)\n",
    "    content = ' '.join(content.split())\n",
    "    \n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def origin_tokenizing(content): #입력 문장 리스트를 토크나이징한다. Kkma는 pos로 분리하고, OS, NNG, NP만 빼내는 것이 좋다.\n",
    "    featurelist = []\n",
    "    tw = Twitter()\n",
    "    for term in tw.pos(content):\n",
    "        if term[1] == 'Noun':\n",
    "            featurelist.append(term[0])\n",
    "        elif term[1] == 'Alpha':\n",
    "            featurelist.append(term[0])\n",
    "        elif term[1] == 'Verb':\n",
    "            featurelist.append(term[0])\n",
    "    return featurelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45 'NoneType' object has no attribute 'decode'\n",
      "141 'NoneType' object has no attribute 'decode'\n",
      "146 'NoneType' object has no attribute 'decode'\n",
      "211 'NoneType' object has no attribute 'decode'\n"
     ]
    }
   ],
   "source": [
    "utf_corpus = []\n",
    "for i, data in enumerate(corpus):\n",
    "    try:\n",
    "        dd = dict()\n",
    "        dd['title'] = data['title'].decode('utf-8')\n",
    "        dd['content'] = data['content'].decode('utf-8')\n",
    "        dd['link'] = data['link'].decode('utf-8')\n",
    "        dd['publish'] = data['publish'].decode('utf-8')\n",
    "        dd['provider'] = data['provider'].decode('utf-8')\n",
    "        dd['tokens'] = origin_tokenizing(preprocessing(dd['content']))\n",
    "    except Exception as err:\n",
    "        print(i, err)\n",
    "    finally:\n",
    "        utf_corpus.append(dd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'content': b'',\n",
       " 'link': b'https://www.cnet.co.kr/view/?no=20150513161050',\n",
       " 'provider': b'cnet',\n",
       " 'publish': None,\n",
       " 'title': b''}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[45]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del utf_corpus[45] #리스트 인덱스 삭제하는법\n",
    "#del utf_corpus[141]\n",
    "#del utf_corpus[146]\n",
    "#del utf_corpus[211]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Docu:\n",
    "    def __init__(self, content, link, provider, publish, title, tokens):\n",
    "        self.content = content\n",
    "        self.link = link\n",
    "        self.provider = provider\n",
    "        self.publish = parse(publish)\n",
    "        self.title = title\n",
    "        self.tokens = tokens\n",
    "    def __repr__(self):\n",
    "        return repr((self.content, self.link, self.provider, self.publish, self.title, self.tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "docments = list()\n",
    "for i, data in enumerate(utf_corpus):\n",
    "    docments.append(Docu(data['content'], data['link'], data['provider'], data['publish'], data['title'], data['tokens']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = sorted(docments, key=lambda docu: docu.publish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015-05-04\n",
      "2015-05-04\n",
      "2015-05-06\n",
      "2015-05-06\n",
      "2015-05-06\n",
      "2015-05-06\n",
      "2015-05-06\n",
      "2015-05-06\n",
      "2015-05-07\n",
      "2015-05-07\n",
      "2015-05-07\n",
      "2015-05-08\n",
      "2015-05-11\n",
      "2015-05-12\n",
      "2015-05-12\n",
      "2015-05-12\n",
      "2015-05-12\n",
      "2015-05-12\n",
      "2015-05-13\n",
      "2015-05-14\n",
      "2015-05-14\n",
      "2015-05-14\n",
      "2015-05-14\n",
      "2015-05-15\n",
      "2015-05-15\n",
      "2015-05-15\n",
      "2015-05-15\n",
      "2015-05-18\n",
      "2015-05-18\n",
      "2015-05-18\n",
      "2015-05-18\n",
      "2015-05-19\n",
      "2015-05-19\n",
      "2015-05-19\n",
      "2015-05-20\n",
      "2015-05-20\n",
      "2015-05-20\n",
      "2015-05-20\n",
      "2015-05-20\n",
      "2015-05-20\n",
      "2015-05-21\n",
      "2015-05-21\n",
      "2015-05-21\n",
      "2015-05-22\n",
      "2015-05-22\n",
      "2015-05-22\n",
      "2015-05-26\n",
      "2015-05-26\n",
      "2015-05-28\n",
      "2015-05-28\n",
      "2015-05-28\n",
      "2015-05-28\n",
      "2015-05-29\n",
      "2015-05-29\n",
      "2015-05-29\n",
      "2015-05-29\n",
      "2015-05-29\n",
      "2015-06-01\n",
      "2015-06-01\n",
      "2015-06-01\n",
      "2015-06-02\n",
      "2015-06-02\n",
      "2015-06-02\n",
      "2015-06-02\n",
      "2015-06-02\n",
      "2015-06-02\n",
      "2015-06-02\n",
      "2015-06-03\n",
      "2015-06-03\n",
      "2015-06-03\n",
      "2015-06-03\n",
      "2015-06-03\n",
      "2015-06-03\n",
      "2015-06-04\n",
      "2015-06-04\n",
      "2015-06-04\n",
      "2015-06-04\n",
      "2015-06-05\n",
      "2015-06-05\n",
      "2015-06-05\n",
      "2015-06-05\n",
      "2015-06-08\n",
      "2015-06-08\n",
      "2015-06-08\n",
      "2015-06-09\n",
      "2015-06-09\n",
      "2015-06-09\n",
      "2015-06-09\n",
      "2015-06-09\n",
      "2015-06-09\n",
      "2015-06-10\n",
      "2015-06-10\n",
      "2015-06-10\n",
      "2015-06-11\n",
      "2015-06-11\n",
      "2015-06-11\n",
      "2015-06-11\n",
      "2015-06-11\n",
      "2015-06-11\n",
      "2015-06-11\n",
      "2015-06-12\n",
      "2015-06-12\n",
      "2015-06-12\n",
      "2015-06-12\n",
      "2015-06-15\n",
      "2015-06-15\n",
      "2015-06-15\n",
      "2015-06-15\n",
      "2015-06-15\n",
      "2015-06-15\n",
      "2015-06-16\n",
      "2015-06-16\n",
      "2015-06-16\n",
      "2015-06-17\n",
      "2015-06-17\n",
      "2015-06-17\n",
      "2015-06-18\n",
      "2015-06-18\n",
      "2015-06-18\n",
      "2015-06-19\n",
      "2015-06-19\n",
      "2015-06-19\n",
      "2015-06-19\n",
      "2015-06-22\n",
      "2015-06-23\n",
      "2015-06-23\n",
      "2015-06-24\n",
      "2015-06-24\n",
      "2015-06-25\n",
      "2015-06-25\n",
      "2015-06-25\n",
      "2015-06-25\n",
      "2015-06-25\n",
      "2015-06-26\n",
      "2015-06-29\n",
      "2015-06-29\n",
      "2015-06-29\n",
      "2015-06-30\n",
      "2015-06-30\n",
      "2015-06-30\n",
      "2015-07-01\n",
      "2015-07-01\n",
      "2015-07-01\n",
      "2015-07-02\n",
      "2015-07-03\n",
      "2015-07-03\n",
      "2015-07-06\n",
      "2015-07-06\n",
      "2015-07-06\n",
      "2015-07-06\n",
      "2015-07-07\n",
      "2015-07-08\n",
      "2015-07-08\n",
      "2015-07-08\n",
      "2015-07-08\n",
      "2015-07-09\n",
      "2015-07-09\n",
      "2015-07-09\n",
      "2015-07-09\n",
      "2015-07-10\n",
      "2015-07-10\n",
      "2015-07-10\n",
      "2015-07-10\n",
      "2015-07-13\n",
      "2015-07-13\n",
      "2015-07-13\n",
      "2015-07-13\n",
      "2015-07-13\n",
      "2015-07-13\n",
      "2015-07-14\n",
      "2015-07-14\n",
      "2015-07-14\n",
      "2015-07-15\n",
      "2015-07-15\n",
      "2015-07-15\n",
      "2015-07-15\n",
      "2015-07-16\n",
      "2015-07-16\n",
      "2015-07-16\n",
      "2015-07-16\n",
      "2015-07-16\n",
      "2015-07-16\n",
      "2015-07-16\n",
      "2015-07-17\n",
      "2015-07-20\n",
      "2015-07-20\n",
      "2015-07-20\n",
      "2015-07-21\n",
      "2015-07-21\n",
      "2015-07-21\n",
      "2015-07-21\n",
      "2015-07-21\n",
      "2015-07-21\n",
      "2015-07-21\n",
      "2015-07-22\n",
      "2015-07-22\n",
      "2015-07-22\n",
      "2015-07-22\n",
      "2015-07-22\n",
      "2015-07-22\n",
      "2015-07-23\n",
      "2015-07-23\n",
      "2015-07-23\n",
      "2015-07-23\n",
      "2015-07-24\n",
      "2015-07-24\n",
      "2015-07-24\n",
      "2015-07-24\n",
      "2015-07-24\n",
      "2015-07-24\n",
      "2015-07-24\n",
      "2015-07-27\n",
      "2015-07-27\n",
      "2015-07-27\n",
      "2015-07-27\n",
      "2015-07-28\n",
      "2015-07-28\n",
      "2015-07-28\n",
      "2015-07-28\n",
      "2015-07-28\n",
      "2015-07-28\n",
      "2015-07-28\n",
      "2015-07-28\n",
      "2015-07-28\n",
      "2015-07-28\n",
      "2015-07-30\n",
      "2015-07-31\n",
      "2015-07-31\n",
      "2015-07-31\n",
      "2015-07-31\n",
      "2015-08-01\n",
      "2015-08-03\n",
      "2015-08-03\n",
      "2015-08-03\n",
      "2015-08-03\n",
      "2015-08-03\n",
      "2015-08-04\n",
      "2015-08-04\n",
      "2015-08-04\n",
      "2015-08-05\n",
      "2015-08-05\n",
      "2015-08-05\n",
      "2015-08-05\n",
      "2015-08-05\n",
      "2015-08-05\n",
      "2015-08-05\n",
      "2015-08-06\n",
      "2015-08-06\n",
      "2015-08-06\n",
      "2015-08-06\n",
      "2015-08-06\n",
      "2015-08-07\n",
      "2015-08-07\n",
      "2015-08-10\n",
      "2015-08-10\n",
      "2015-08-10\n",
      "2015-08-11\n",
      "2015-08-11\n",
      "2015-08-11\n",
      "2015-08-12\n",
      "2015-08-12\n",
      "2015-08-13\n",
      "2015-08-13\n",
      "2015-08-13\n",
      "2015-08-14\n",
      "2015-08-14\n",
      "2015-08-17\n",
      "2015-08-19\n"
     ]
    }
   ],
   "source": [
    "#정렬 확인\n",
    "#for data in result:\n",
    "#    print(data.publish.date())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "predata = result[0]\n",
    "datecorpus = {}\n",
    "datecorpus[str(predata.publish.date())] = {'string' : predata.content + \"\\n\"}\n",
    "\n",
    "for i in range(1, len(result)):\n",
    "    if result[i].publish.date() == predata.publish.date():\n",
    "        try:\n",
    "            datecorpus[str(result[i].publish.date())]['string'] += str(result[i].content + \"\\n\")\n",
    "            #print(result[i].publish.date())\n",
    "        except KeyError as e:\n",
    "            datecorpus[str(result[i].publish.date())] = {'string' : str(result[i].content + \"\\n\")}\n",
    "            #print(e)\n",
    "    predata = result[i]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword = set([('있', 'Verb'), ('하', 'Verb'), ('할', 'Verb'), ('되', 'Verb'), ('없', 'Verb'), \n",
    "                ('위해', 'Noun'), ('수', 'Noun'), ('있다', 'Adjective'),('없다', 'Adjective'), ('통해', 'Noun'), \n",
    "                ('그럴', 'Adjective'), ('를', 'Noun'), ('는', 'Noun'), ('때문', 'Noun'), ('와', 'Noun'), ('전혀', 'Noun')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 65.5 ms\n",
      "Wall time: 92.2 ms\n",
      "Wall time: 39.6 ms\n",
      "Wall time: 73.4 ms\n",
      "Wall time: 59.5 ms\n",
      "Wall time: 42.2 ms\n",
      "Wall time: 79.9 ms\n",
      "Wall time: 28.3 ms\n",
      "Wall time: 160 ms\n",
      "Wall time: 24.8 ms\n",
      "Wall time: 31.7 ms\n",
      "Wall time: 14.9 ms\n",
      "Wall time: 37.2 ms\n",
      "Wall time: 98.2 ms\n",
      "Wall time: 46.6 ms\n",
      "Wall time: 128 ms\n",
      "Wall time: 68.9 ms\n",
      "Wall time: 61.5 ms\n",
      "Wall time: 24.8 ms\n",
      "Wall time: 59 ms\n",
      "Wall time: 249 ms\n",
      "Wall time: 46.1 ms\n",
      "Wall time: 143 ms\n",
      "Wall time: 49.6 ms\n",
      "Wall time: 124 ms\n",
      "Wall time: 38.7 ms\n",
      "Wall time: 73.9 ms\n",
      "Wall time: 23.8 ms\n",
      "Wall time: 74.4 ms\n",
      "Wall time: 33.7 ms\n",
      "Wall time: 56.5 ms\n",
      "Wall time: 125 ms\n",
      "Wall time: 36.2 ms\n",
      "Wall time: 114 ms\n",
      "Wall time: 184 ms\n",
      "Wall time: 9.92 ms\n",
      "Wall time: 51.1 ms\n",
      "Wall time: 47.6 ms\n",
      "Wall time: 70.4 ms\n",
      "Wall time: 31.7 ms\n",
      "Wall time: 55.1 ms\n",
      "Wall time: 54.5 ms\n",
      "Wall time: 65 ms\n",
      "Wall time: 178 ms\n",
      "Wall time: 26.3 ms\n",
      "Wall time: 89.3 ms\n",
      "Wall time: 53.6 ms\n",
      "Wall time: 57 ms\n",
      "Wall time: 103 ms\n",
      "Wall time: 124 ms\n",
      "Wall time: 104 ms\n",
      "Wall time: 38.6 ms\n",
      "Wall time: 113 ms\n",
      "Wall time: 47.1 ms\n",
      "Wall time: 101 ms\n",
      "Wall time: 136 ms\n",
      "Wall time: 105 ms\n",
      "Wall time: 29.3 ms\n",
      "Wall time: 58.6 ms\n",
      "Wall time: 16.9 ms\n",
      "Wall time: 42.7 ms\n",
      "Wall time: 64.5 ms\n"
     ]
    }
   ],
   "source": [
    "sorted_word = {}\n",
    "for date in list(datecorpus.keys()):\n",
    "    tr = TextRank(window=1, coef=1, threshold = 5)\n",
    "    %time tr.load(RawTagger(datecorpus[date]['string']), lambda w: w not in stopword and (w[1] in ('Noun', 'Adjective', 'Alpha', 'Number')))\n",
    "    tr.build()\n",
    "    datecorpus[date]['pmi'] = tr.extract(0.4)\n",
    "    for data in datecorpus[date]['pmi']:\n",
    "        try:\n",
    "            sorted_word[data[0]][date] = data[1]\n",
    "        except KeyError as e:\n",
    "            sorted_word[data[0]] = {}\n",
    "            sorted_word[data[0]][date] = data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6135"
      ]
     },
     "execution_count": 482,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sorted_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\na = list(datecorpus.keys())\\npredate = datecorpus[a[0]]\\nfor i in range(1, len(a)):\\n    for c_word in datecorpus[a[i]]['pmi']:\\n        for j, pre_c_word in enumerate(predate['pmi']):\\n            if str(pre_c_word[0]) == str(c_word[0]):\\n                del predate['pmi'][j]\\n    datecorpus[a[i]]['pmi'] += predate['pmi']\\n    #\\n    print(len(datecorpus[a[i]]['pmi']))\\n    predate = a[a[i]]\\n\""
      ]
     },
     "execution_count": 426,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#다시 짜자 좀 쉬고...\n",
    "'''\n",
    "a = list(datecorpus.keys())\n",
    "predate = datecorpus[a[0]]\n",
    "for i in range(1, len(a)):\n",
    "    for c_word in datecorpus[a[i]]['pmi']:\n",
    "        for j, pre_c_word in enumerate(predate['pmi']):\n",
    "            if str(pre_c_word[0]) == str(c_word[0]):\n",
    "                del predate['pmi'][j]\n",
    "    datecorpus[a[i]]['pmi'] += predate['pmi']\n",
    "    #\n",
    "    print(len(datecorpus[a[i]]['pmi']))\n",
    "    predate = a[a[i]]\n",
    "'''\n",
    "#시간 순에 따른 단어의 PMI 정렬은 한 그래프안의 표현하는데 문제가 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import font_manager, rc #TSNE 그림에서 한글이 깨지는 문제를 해결해준다.\n",
    "font_fname = 'C:\\\\windows\\\\Fonts\\\\\\x7f\\x7f\\x7f\\x7fBOLD.TTF'     # A font of your choice\n",
    "font_name = font_manager.FontProperties(fname=font_fname).get_name()\n",
    "rc('font', family=font_name)\n",
    "plt.rcParams['axes.grid'] = True\n",
    "plt.rcParams[\"figure.figsize\"] = (80,40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = list(sorted_word.keys())\n",
    "#date = list(datecorpus.keys())\n",
    "for i, word in enumerate(sorted_word):\n",
    "    plt.plot(sorted_word[a[i]].keys(), sorted_word[a[i]].values(), 'bs--' ,label=word)\n",
    "    #plt.legend(loc='upper left')\n",
    "    #plt.xlim(date[0], date[-1])\n",
    "    plt.xticks(list(datecorpus.keys()))\n",
    "    plt.ylim(-5, 20)\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Complex Word PMI')\n",
    "    plt.title(word)\n",
    "    fig = plt.gcf() #변경한 곳\n",
    "    fig.savefig('./word_graph/'+word+'.png') #변경한 곳\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a08c465358>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot([4, 6], [5, -1], 'bs--')\n",
    "plt.xticks([0,1,2,3,4,5,6,7,8,9,10])\n",
    "plt.ylim(-5, 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
